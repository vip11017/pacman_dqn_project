{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on Ms Pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The project is going to be a mimic of the game Pac-Man. The goal is to have Pac-Man eat all the yellow pellets, big pellets, and fruit. This should be done while avoiding being eaten by the ghosts. If a maze is cleared, a new maze will be generated, and the score will keep running. Pac-Man will also have three lives, and he will lose a life each time he touches a ghost. The game is over when all three lives have been lost. When a big pellet is eaten, Pac-Man is allowed to eat the ghosts for an added number of points. No data is necessary for this project to be completed. The success will be measured by the number of points a player earns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Pac-Man is a classic example of a maze-based game with dynamic enemy behaviors and strategic gameplay events, so it has naturally been a popular game to test AI research on. Additionally, research related to AI-based gameplay has seen significant improvements in recent years. \n",
    "Prior work has utilized various AI algorithms and techniques to create a bot that can optimally beat the game and obtain a high score. One barrier that made Pac-Man a difficult problem to solve was the constantly changing boards and moving parts. The topic was explored by researchers at the University of Loannina who proposed that an informative state space description was important to help design efficient RL agents needed to solve Pac-Man and all games similar.<a name=\"ms.pac-man\"></a>[<sup>[1]</sup>](#ms.pac-man)\n",
    "Researchers from Maluuba, a startup that was acquired by Microsoft, used reinforcement learning to achieve the maximum score possible of 999,990 on the Atari 2600 version of Ms. Pac-Man<a name=\"linn\"></a>[<sup>[2]</sup>](#linn). The team essentially divided this into small problems to solve and then split these into tasks that were eventually completed by AI agents. This process, known as Hybrid Reward Architecture, used over 150 different AI agents to fully beat Ms. Pac-Man<a name=\"linn\"></a>[<sup>[2]</sup>](#linn). The belief was that this form of reinforcement learning would allow these agents to “think” and make their own decisions, which would allow engineers to work on more complex and seemingly higher-value projects and tasks. This sort of reinforcement learning isn’t the only thing to be explored; Deep Reinforcement Learning from Juman Feedback (DRLHF), as well as Cooperative Inverse Reinforcement Learning (CIRL), have both been explored to allow for an AI-controlled Pac-Man to learn from extremely limited human feedback and find an optimal solution to the game<a name=\"box\"></a>[<sup>[3]</sup>](#box). DRLHF has many applications, and it allows the Pac-Man agent to first learn from trained human players, interact with the environment on its own, and be shaped by the human feedback it receives when exploring the game environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are trying to solve the problem of making Pacman fully autonomous using AI algorithms and techniques. This would mean we want Pacman to fully decide, based on current environmental conditions, what the next steps should be and to maximize its score by getting as many points as possible before beating the game or dying 3 times. To do this, we would utilize AI algorithms such as the Markov Decision Process to find the most optimal next action, while also using some kind of search algorithms to search for the ghosts and coins in the maze. This task can be quantifiable by representing the board as our environment with $n$ number of configurations or states. The action space of the Pacman would be 4 cardinal directions with some constrictions depending on the surrounding environment. The problem can be measured through the number of total points obtained at the end of the game. The metric we can use is the higher the score (number of points) the more successful the AI performed on the task, and should be optimized to keep reaching this score or higher. This problem is replicable since in each game of Pacman you play the same problem occurs, where you are trying to get Pacman to beat the game or to get the most points possible before dying. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In Pacman, the rewards could be determined by the rules and structure of the game. We could also use the same deduction and addition of points to determine the reward function. The following reward breakdown is as follows: Pac-dot is +10 points, Power Pellet is +50 points, 1st Ghost is +200 points, 2nd Ghost is +400 points, 3rd Ghost is +800 points, 4th Ghost is 1600 points, Cherry is +100 points, Strawberry is +300 points, Orange is +500 points, Apple is +700 points, Melon is +1000 points, Galaxian is +2000 points, Bell is +3000 points, and key is +5000 points. To help guide Pacman in not dying from touching Ghosts, the act of colliding with the ghost can have a large negative reward such as -100 points. To encourage Pacman to make optimal decisions, but also care more about surviving than taking risks to get a higher score, we can implement a small reward of around +1 for each step Pacman takes that keeps it alive. To encourage Pacman to finish the maze, we can have a large positive reward to finish the maze of +500 points. Based on this reward mechanism, the implemented Pacman would be encouraged to take optimal strategies to stay alive and go for higher rewarded points, while also staying alive and finishing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To achieve full autonomy for Pac-man, we will employ AI algorithms like Deep Q Learning with experience replay to be able to help Pacman learn some patterns. Since we are using the OpenAI MsPacman environment, the observation space is a 3D pixel image representing the (x,y,(RGB) value as the observation space. Due to this nature of using images as the state space, we have to use neural networks to process this data and be able to help interpret which action is best for Pacman. To help with processing this data to make it easier for the neural network to understand the image, we would first need to preprocess the image. This requires us to downsample the image from 210 x 160 pixels with RGB values down to 110 x 84 image. After downsampling this, we can crop the image down to an 84 x 84 region and convert the image to a grayscale. Now you can stack 4 images together to understand the general direction of where Pacman is going because just from one screenshot you can’t tell where any of the agents and ghosts are moving. This is all implemented through the Open AI gym wrappers for preprocessing and vertical stacking.\n",
    "\n",
    "We will choose DQN for Pacman because this algorithm is able to handle high dimensional observation space (image input), and then use a function approximator to be able to find the action that best correlates to the state at the time. This algorithm will offer some advantages to others which are that DQN is able to use many weights to factor into the neural network when deciding the action. Also, by using the experience replay buffer, it can learn from many samples, and by having these samples randomized it can remove any correlation which reduces some of the variance. The experience replay also allows the model to use its past relatively recent experiences to be able to solve the problem and keep Pacman alive as long as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "One evaluation metric we can use to measure the performance of the benchmark and solution model is the total cumulative reward. This cumulative reward would be the sum of all the rewards for each model’s iterations. For each episode in the model, we can keep track of the cumulative reward where a higher number would mean a better performance by the AI, and a lower score would mean a worse performance. We can use these total cumulative rewards to compare the performance and how successful the model is. For example, If the benchmark model has a total reward of roughly 100, while the solution model has a total reward of roughly 10, this could indicate that the benchmark model is better at collecting more points with fewer consequences, while the solution model tends to face more consequences of undesired actions. \n",
    "Another evaluation metric that can be used to measure the performance of the benchmark and solution model is the average score per game. This average score per game can be formulated by the formula: $\\frac{1}{N}\\sum_{i=1}^N E_i$, where N is the total number of games and $E_i$ would be the ith episode. The score of the game could be captured by the number of coins and points the user gets in one game. By capturing the average score per game, we can compare it with other models and determine that a higher average score means that the model performed relatively better, while lower scores indicate the model performed relatively worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Importing Libraries\n",
    "We will be using the `gymnasium` enviroment as our enviroment for MsPacman. To help with some preprocessing, we will be utilizing the `gymnasium.wrappers`AtariPreprocessing and Framestack to preprocess the observations for ease of use. For your neurel network we will be using Pytorch, and in `torch`, we will use `torch.nn`, `torch.optim`, and `torch.nn.function`. For some of the behind the scenes math we will use `numpy` and `random`. For the Replay Buffer we will use a `deque` and `namedtuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using mps\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'You are using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters/Initializing the environment\n",
    "\n",
    "Here we define the hyperparameters and the environment for future use. We will be using the gymnasium library MsPacman environment from the Atari Library. To perform the preprocessing part of the algorithm we use the gymnasium wrappers AtariPreprocessing and Framestack to preprocess the frames of the observation space and to stack 4 frames together into one observation space. \n",
    "- `state_size`: observation space\n",
    "- `action_size`: number of actions\n",
    "- `buffer_size`: size of replay buffer deque\n",
    "- `batch_size`: the number of experiences from the replay buffer that will be resamples and then updated to the neural network\n",
    "- `tau`: soft update parameter for updating the target network and controls how much of the weights in the neural network is transferred to the target network at each step\n",
    "- `n_episodes`: number of episodes for training\n",
    "- `max_t`: maximum number of timesteps in the training loop for each episode\n",
    "- `eps_start`: the initial epsilon value for epsilon-greedy algorithm for choosing action\n",
    "- `eps_end`: the minimum value of epsilon for the epsilon greedy algorithm\n",
    "- `eps_decay`: the decay rate of epislon value where as episodes increase,t he epsilon value decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# Define the MsPacman environment with render_mode\n",
    "env = gym.make('ALE/MsPacman-v5', render_mode='rgb_array')\n",
    "env = AtariPreprocessing(env, frame_skip=1)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "# Hyperparameters\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "buffer_size = 100000\n",
    "batch_size = 32\n",
    "tau = 0.001\n",
    "n_episodes = 1000\n",
    "max_t = 1000\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = (eps_end/eps_start)**(1/n_episodes)\n",
    "\n",
    "# Normalize function for preprocessing state\n",
    "def normalize(state):\n",
    "    return np.array(state) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork\n",
    "\n",
    "This DQN approach is the backbone of the algorithm which uses the `nn.Module` parent class. For our neural network we decided to use a CNN due to the nature of the observation space, which are images. The convolutional layer takes in a 2D image where it inputs an image with 4 input channels (4 stacked frames). It then goes through all of these layers of the neural network and the output will be the action size where each output channel corresponds to a specific action. The forward function is a function used in PyTorch where it inputs each “frame” or state into the layers to output the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Neural Network for approximating Q-values.\"\"\"\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_input_size = self._get_conv_output(state_shape)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_output(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = self.conv_layers(o)\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.conv_layers(state)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "This replay buffer is designed to store the experiences in a tuple with the format of (states, actions, rewards, next_states, dones). It uses the hyperparameters discussed previously which are: `action_size`, `buffer_size`, `batch_size`, and then uses a `namedtuple` to store the tuple values for the memory. The add method is responsible for the append feature where it will keep updating the memory with new experiences. The sample method will take a random batch_size subset of all the tuples stored in memory and then returns a tuple with each of the stored values (states, actions, rewards, next_states, dones). This sample method converts the list into a numpy array and then into a PyTorch tensor for each of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.array([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.array([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent\n",
    "\n",
    "The DQN agent is the main agent that interacts with all the other parts of the algorithm classes to create the agent. It utilizes neural networks defined as `qnetwork_local` and `qnetwork_traget`. It also uses the Adam optimizer to be able to optimize its values based on the performances of the local Q-network. The memory part of it incorporates the ReplayBuffer class as shown previously, and t_step is the step counter to control the frequency of learning steps. The step method is called for each action taken by Pacman. This step function controls the frequency of learning where it is every 4 steps. It incorporates the memories to be able to take the next steps. The act method is when the agent takes an action, this is based on the neural network value that is returned based on the given current state. It also incorporates the epsilon greedy algorithm to determine the next action. The learn method updates the local Q-network with the most recent experience every 4 steps (from step method) and uses a batch of experience to find the specific values to update. The soft_update method updates the parameters of the target Q-network towards the local Q-network parameters while using the hyperparameter value of tau to mix the local and target parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters())\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.memory.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma=0.99)\n",
    "\n",
    "    def act(self, state, eps=0.99):\n",
    "        state = torch.from_numpy(normalize(state)).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Compute Q targets for next states\n",
    "        q_target_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_target = rewards + (gamma * q_target_next * (1 - dones))\n",
    "\n",
    "        # Compute Q expected for current states\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_expected, q_target)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network parameters\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "This part of the code is responsible for training the algorithm. We first define two functions that will save and load our checkpoints. Since this approach takes a long time for the algorithm to train, we want to save each episode in case some problems or errors occur during the training process. This will help reduce the training iteration by instead of running the computer for a long duration of time, we can split it up into sections and reduce the time The dqn function is the main part of the training loop where it trains a `n_espisodes` number of episodes and for each episodes runs until it reaches the max time step or Pacman dies. It runs through these episodes while also constantly updating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save necessary torch files to future training\n",
    "def save_checkpoint(agent, filename='checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'qnetwork_local_state_dict': agent.qnetwork_local.state_dict(),\n",
    "        'qnetwork_target_state_dict': agent.qnetwork_target.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict()\n",
    "    }, filename)\n",
    "\n",
    "#Function to load necessary torch files for training or using pre-trained model\n",
    "def load_checkpoint(agent, filename='checkpoint.pth'):\n",
    "    checkpoint = torch.load(filename)\n",
    "    agent.qnetwork_local.load_state_dict(checkpoint['qnetwork_local_state_dict'])\n",
    "    agent.qnetwork_target.load_state_dict(checkpoint['qnetwork_target_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=500, max_t=1000, eps_start=1, eps_end=0.01, eps_decay=(0.01/1)**(1/500)):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    moving_avg_scores = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _ = env.reset()  # Unpack the state from the reset method\n",
    "        state = normalize(state)\n",
    "        score = 0\n",
    "        lives = env.unwrapped.ale.lives()  # Get the initial number of lives\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Unpack the state from the step method\n",
    "            next_state = normalize(next_state)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # Check if lives have changed\n",
    "            if env.unwrapped.ale.lives() < lives:\n",
    "                lives = env.unwrapped.ale.lives()  # Update the number of lives\n",
    "                if lives == 0:  # If all lives are lost, break the loop\n",
    "                    done = True\n",
    "                else:\n",
    "                    done = False\n",
    "            if done:\n",
    "                print(f\"Took {t} steps until Agent ran out of lives\")\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)\n",
    "        moving_avg_scores.append(np.mean(scores_window))\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        print(f'Episode {i_episode}, Avg Score: {np.mean(scores_window)}')\n",
    "        #termination early condition\n",
    "        if np.mean(scores_window) >= 1000.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_window)}')\n",
    "            save_checkpoint(agent, \"checkpoint.pth\")\n",
    "            break\n",
    "\n",
    "    save_checkpoint(agent, \"checkpoint.pth\")\n",
    "    return scores, moving_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, moving_avg_scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(np.arange(len(scores)), scores, label='Scores', color='blue')\n",
    "    ax.plot(np.arange(len(moving_avg_scores)), moving_avg_scores, label='Moving Average Scores', color='red')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "    fig.savefig('training_scores.png')\n",
    "\n",
    "def create_video(agent, env, filename=\"mspacman.mp4\"):\n",
    "    video_env = gym.wrappers.RecordVideo(env, './video', episode_trigger=lambda x: True, video_length=0)\n",
    "    state, _ = video_env.reset()\n",
    "    state = normalize(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, eps=0.0)  # Use greedy policy for evaluation\n",
    "        state, reward, done, _, _ = video_env.step(action)\n",
    "        state = normalize(state)\n",
    "    video_env.close()\n",
    "    video_dir = './video'\n",
    "    video_file = [f for f in os.listdir(video_dir) if f.endswith('.mp4')][0]\n",
    "    os.rename(os.path.join(video_dir, video_file), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "#Uncomment below code if you want to run the code with pretrained-model\n",
    "#load_checkpoint(agent, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the training DQN training loop (uncomment to run training)\n",
    "#scores, moving_avg_scores = dqn(n_episodes, max_t, eps_start, eps_end, eps_decay)\n",
    "\n",
    "# Plots the scores and moving averages (uncomment if you ran training loop)\n",
    "#plot_scores(scores, moving_avg_scores)\n",
    "\n",
    "#Creates a video rendering of the agent to see how it performs\n",
    "#create_video(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "<img src=\"training_scores.png\" alt=\"Training Plot\" />\n",
    "\n",
    "From the results above, the main takeaway that’s seen is that there seems to be a tentative ceiling for the average high score. The results clearly show that as the number of episodes increases, the score the agent finishes with is 700 on average. However, there is a lot of volatility with the scores, as the score 700 exactly doesn’t seem to be where the game stops. This can be attributed to a couple of reasons. First, that score can be based only on the factors that came with this environment. The specific environment, such as the maze layout and position of different pellets, significantly impacts the agent’s performance. The positions of the ghosts provide some variability as well. This is why this score cannot be seen as the average for all Pac-Man games or different versions of the game. Second, the algorithm used to train the Pac-Man agent may come with its limitations. Since the epsilon-greedy learning model was used, some of the movements were random and may not have been the most optimal move when looking in the long run. This is also why there are times when the agent scored much higher or lower than the average; the consistency was not present. Additionally, a single set of hyperparameters was used, which also likely influenced the results. Through this diagram, humans can learn how there is some sort of predictable pattern and aproach that one can learn when playing Pacman. If the graph of pacman did not steadily increase or converge at a point, we could interpret this as there is no predictable pattern to Pacman, however, evidently Pacman is able to predict certain moves which allows him to gain more points and survive longer, thus showing the steady increase in total cumaltive rewards and the moving average of the most recent 100 rewards.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "One thing that could have been done to explore and potentially improve the agent behavior was further experimentation with the hyperparameters, as previously mentioned. The set of values used were typically used for most Atari games, so the thought was to stick to these same values under the assumption that they would produce the best results. However, changing these values may have led to a more consistent score distribution, even if the average score stays the same. It would allow us to say more confidently that a score of 700 can be achieved. Another huge limitation is that the model takes hours to be trained, as our laptops don’t have enough computational power to run it more quickly. As a result, there are only so many times the code can be run in a day, which provides less time to try to use a more efficient algorithm to train the agent. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Seeing as no data is going to be used, there aren’t any major concerns with ethics or data privacy for this project. A potential concern would be the AI recording data related to how a player plays the game, but even this isn’t a major concern because the AI will likely be implemented using a separate algorithm. \n",
    "A potential concern is algorithmic bias. Seeing as the AI algorithms used in this game will be written by people, we will likely introduce our own biases into the algorithms, which may lead to the AI-controlled Pac-Man favoring a certain direction, part of the gameboard, etc. \n",
    "Another concern is the subject of intellectual property. From a general sense, there isn’t much separating this game from the original Pac-Man or any other Pac-Man games that are currently available for download. This game, however, will not be available for purchase and is for use in this project, and it has already been acknowledged that this game is a copy of the original game. \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The primary takeaway from this study is that the average high score for the Pac-Man agent plateaus at approximately 700, with notable volatility around this average. This ceiling is influenced by the specific environment settings, such as maze layout and ghost positions, and the limitations of the epsilon-greedy learning model, which incorporates randomness in agent movements. The findings highlight the importance of environment-specific factors and the potential need for hyperparameter tuning to achieve consistent performance.\n",
    "In the broader context of AI research related to the gaming industry, this work emphasizes the challenges of achieving consistently high performance in complex environments, as well as the necessity for continuous algorithmic and parameter optimization. Similar to those who researched Pac-Man and other Atari games, as well as those who have conducted research on DQN, these findings show that it is tough to find the perfect balance between exploration and exploitation, as well as manage the complexity between the environment and the states it consists of. Future work could focus on experimenting with different hyperparameters and exploring more sophisticated training algorithms to enhance agent performance. Additionally, improving the state representation by incorporating more detailed features of the game environment could be tested to see if the agent will make more informed decisions that lead to optimal results. Finally, using real-world data to train the Pac-man agent can also be explored, although this may introduce ethical concerns to some extent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"ms.pac-man\"></a>1.[^](#ms.pac-man): Play Ms. Pac-Man Using an Advanced Reinforcement Learning, [www.lix.polytechnique.fr/~ntziortziotis/pubs/SETN14.pdf](https://www.lix.polytechnique.fr/~ntziortziotis/pubs/SETN14.pdf)<br> \n",
    "<a name=\"linn\"></a>2.[^](#linn): Linn, A. (14 June 2017) Divide and conquer: How mnicrosoft researchers used AI to master Ms. Pac-Man. Microsoft Blogs https://blogs.microsoft.com/ai/divide-conquer-microsoft-researchers-used-ai-master-ms-pac-man/#:~:text=Microsoft%20researchers%20have%20created%20an,tasks%20that%20augment%20human%20capabilities.<br>\n",
    "<a name=\"box\"></a>3.[^](#box): Box, C. (8 Aug 2023) Pac-Man as an AI Agent: A Blast from the Past Aesthetic in the Reinforcement Learning from Human Feedback Process, Medium, https://medium.com/@celeste_box/pac-man-as-an-ai-agent-a-blast-from-the-past-aesthetic-in-the-reinforcement-learning-from-human-39a1fb6de624 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
